{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Household Sounds Capstone\n",
    "## 4. Pre-Processing: Convolutional Neural Network\n",
    "- In this notebook we will create a Validation and Test datasets for hyperparameter tuning of the Neural Networks. \n",
    "- We will also perform some minor data cleaning to prepare the data for modeling in the next notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional Neural Network Validation-Test Split\n",
    "- For the Convolutional Neural Network models, we will split the Testing data into two equal sized datasets. \n",
    "- Validation set will be used to evaluate model perforamnce while hyperparameter tuning. \n",
    "- Test set will only be used to evaluate the final models. \n",
    "- Training dataset has already been separated by the creator's of this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading metadata\n",
    "dev_info = pd.read_json('data/labelled_dev_info.json')\n",
    "eval_info = pd.read_json('data/labelled_eval_info.json')\n",
    "dev_info_resampled = pd.read_json('data/dev_info_resamp.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5115, 12)\n",
      "(5116, 12)\n"
     ]
    }
   ],
   "source": [
    "# shuffling eval_info indices before splitting into validation/test datasets\n",
    "shuffled_eval_info = eval_info.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "#splitting into validation and test datasets (50-50 split)\n",
    "validation_info = shuffled_eval_info.iloc[:5115]\n",
    "test_info = shuffled_eval_info.iloc[5115:]\n",
    "\n",
    "print(validation_info.shape)\n",
    "print(test_info.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffled_eval_info.to_json('data/shuffled_eval_info.json')\n",
    "validation_info.to_json('data/validation_info.json')\n",
    "test_info.to_json('data/test_info.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Pre-Processing Mean MFCC Value Features\n",
    "- Scaling Mean MFCC Values to zero mean and unit variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40000, 32)\n",
      "(10231, 32)\n",
      "(40000,)\n",
      "(10231,)\n"
     ]
    }
   ],
   "source": [
    "# loading Training and Testing Data features: Numpy Arrays of Mean MFCC values\n",
    "X_train = np.load('data/train_resamp_mean_mfcc_values.npz')['arr_0']\n",
    "X_test = np.load('data/test_augmented_mean_mfcc_values.npz')['arr_0']\n",
    "\n",
    "# declaring target variable:\n",
    "y_train = dev_info_resampled['labels'].to_numpy()\n",
    "y_test = eval_info['labels'].to_numpy()\n",
    "\n",
    "# Standardizing/normalizing features \n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.fit_transform(X_test)\n",
    "\n",
    "\n",
    "np.savez('data/scaled_train_mean_mfcc_values.npz', X_train)    \n",
    "np.savez('data/scaled_test__mean_mfcc_values.npz', X_test)  \n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation-Test Split\n",
    "- The Nueral Network model requires the same validation-test data split with a different set of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5115, 33)\n",
      "(5116, 33)\n"
     ]
    }
   ],
   "source": [
    "# making validation-test split for Neural Network hyperparameter tuning\n",
    "val_test_info= pd.DataFrame(X_test)\n",
    "val_test_info['labels'] = y_test\n",
    "\n",
    "# shuffling before splitting into validation/test datasets\n",
    "val_test_info = val_test_info.sample(frac=1).reset_index(drop=True)\n",
    "validation_info = val_test_info.iloc[:5115]\n",
    "test_info = val_test_info.iloc[5115:]\n",
    "\n",
    "validation_info.to_json('data/nn_validation_info.json')\n",
    "test_info.to_json('data/nn_validation_info.json')\n",
    "\n",
    "print(validation_info.shape)\n",
    "print(test_info.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning Dataframes Before Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading data\n",
    "eval_info = pd.read_json('data/labelled_eval_info.json')\n",
    "validation_info= pd.read_json('data/validation_info.json')\n",
    "test_info = pd.read_json('data/test_info.json')\n",
    "\n",
    "# Removing unnecessary columns used during EDA\n",
    "eval_info= eval_info.drop(columns=['title','license','uploader','wav_name','labels_15','labels_2','labels_4'])\n",
    "eval_info.to_json('data/labelled_eval_info.json')\n",
    "test_info = test_info.drop(columns=['title','license','uploader','wav_name','labels_15','labels_2','labels_4'])\n",
    "test_info.to_json('data/test_info.json')\n",
    "validation_info = validation_info.drop(columns=['title','license','uploader','wav_name','labels_15','labels_2','labels_4'])\n",
    "validation_info.to_json('data/validation_info.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next Step: Modeling \n",
    "- In the next notebook, we will begin training one CNN with the Mel-Frequency Sprectrograms, one CNN with Mel-Frequency Cepstrum Spectrograms, and series of classifiers trained with the Mean Mel-Frequency Cepstrum Coefficients."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
