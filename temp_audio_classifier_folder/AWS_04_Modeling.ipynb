{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[dataset: FSD50k (Free Sound Dataset 50k)](https://zenodo.org/record/4060432#.X5ySHHhKgWo)\n",
    "\n",
    "\n",
    "Thesis: Classify raw audio files into categories associated with household noises to identify emergency related sounds(humans (voice), animals, instruments, water, door, car, explosion (boom, thunder).\n",
    "\n",
    "### 3 Main CNN Models:\n",
    "\n",
    "1. CNN with Wave Spectrograms\n",
    "2. CNN with MFCC Spectrograms\n",
    "3. CNN with Mean Values of MFCC Features\n",
    "\n",
    "for graphing keras model results\n",
    "\n",
    "also start random forest some point soon (or xgboost? or both?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# list = os.listdir('/home/ec2-user/SageMaker/data/train_mfcc') # dir is your directory path\n",
    "# number_files = len(list)\n",
    "# print(number_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run with --yes\n",
    "# !conda install -c conda-forge librosa "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import librosa\n",
    "import librosa.display\n",
    "import IPython.display as ipd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import skimage.io\n",
    "import os\n",
    "import pickle\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n",
    "from keras.utils import to_categorical\n",
    "from keras import optimizers\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_checker(track_num):\n",
    "    pred_index = np.argmax(preds[track_num])\n",
    "\n",
    "    # checking what the audio file is\n",
    "    print(\"Description of Audio file:\", df_2000.iloc[track_num]['description'])\n",
    "\n",
    "    print('\\nPredicted Class:', label_dict[pred_index],'\\nTrue Class:', label_dict[df_2000.iloc[track_num]['labels']])\n",
    "\n",
    "    # Lets play the audio \n",
    "\n",
    "    file_path = audio_dir + list_of_wav_names[track_num]\n",
    "    data, sr = librosa.load(file_path)\n",
    "\n",
    "    sound = ipd.Audio(file_path)\n",
    "    return sound\n",
    "\n",
    "# pred_checker(44)\n",
    "# #  for fuller track info: df_2000.iloc[track_num]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Meta Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_info = pd.read_json('data/labelled_dev_info.json')\n",
    "eval_info = pd.read_json('data/labelled_eval_info.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>tags</th>\n",
       "      <th>license</th>\n",
       "      <th>uploader</th>\n",
       "      <th>track_num</th>\n",
       "      <th>wav_name</th>\n",
       "      <th>png_name</th>\n",
       "      <th>labels_15</th>\n",
       "      <th>labels_2</th>\n",
       "      <th>labels_4</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>RalfHutterWorking.wav</td>\n",
       "      <td>Ralf Hutter from Kraftwerk saying \"Working on ...</td>\n",
       "      <td>[male, voice]</td>\n",
       "      <td>http://creativecommons.org/licenses/by/3.0/</td>\n",
       "      <td>fectoper</td>\n",
       "      <td>63</td>\n",
       "      <td>63.wav</td>\n",
       "      <td>63.png</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>keyboard-rhymtic.wav</td>\n",
       "      <td>Noise of an average logitech keyboard. Pretty ...</td>\n",
       "      <td>[keyboard, rhythmic, tap, type]</td>\n",
       "      <td>http://creativecommons.org/licenses/by/3.0/</td>\n",
       "      <td>Anton</td>\n",
       "      <td>136</td>\n",
       "      <td>136.wav</td>\n",
       "      <td>136.png</td>\n",
       "      <td>11</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     title                                        description  \\\n",
       "63   RalfHutterWorking.wav  Ralf Hutter from Kraftwerk saying \"Working on ...   \n",
       "136   keyboard-rhymtic.wav  Noise of an average logitech keyboard. Pretty ...   \n",
       "\n",
       "                                tags  \\\n",
       "63                     [male, voice]   \n",
       "136  [keyboard, rhythmic, tap, type]   \n",
       "\n",
       "                                         license  uploader  track_num  \\\n",
       "63   http://creativecommons.org/licenses/by/3.0/  fectoper         63   \n",
       "136  http://creativecommons.org/licenses/by/3.0/     Anton        136   \n",
       "\n",
       "    wav_name png_name  labels_15  labels_2  labels_4  labels  \n",
       "63    63.wav   63.png          0         0         0       0  \n",
       "136  136.wav  136.png         11         8         3       1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_info.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>tags</th>\n",
       "      <th>license</th>\n",
       "      <th>uploader</th>\n",
       "      <th>track_num</th>\n",
       "      <th>wav_name</th>\n",
       "      <th>png_name</th>\n",
       "      <th>labels_15</th>\n",
       "      <th>labels_2</th>\n",
       "      <th>labels_4</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>391277</th>\n",
       "      <td>Spring Birds Forest 04 Amp.wav</td>\n",
       "      <td>An other birds singings recorded on the mornin...</td>\n",
       "      <td>[birdsong, bird, forest, environment, morning,...</td>\n",
       "      <td>http://creativecommons.org/publicdomain/zero/1.0/</td>\n",
       "      <td>ANARKYA</td>\n",
       "      <td>391277</td>\n",
       "      <td>391277.wav</td>\n",
       "      <td>391277.png</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>392115</th>\n",
       "      <td>Snap of fingers</td>\n",
       "      <td>a snap of one's fingers</td>\n",
       "      <td>[fingers, finger, 5maudio17, uam, fingersnap]</td>\n",
       "      <td>http://creativecommons.org/publicdomain/zero/1.0/</td>\n",
       "      <td>edton</td>\n",
       "      <td>392115</td>\n",
       "      <td>392115.wav</td>\n",
       "      <td>392115.png</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 title  \\\n",
       "391277  Spring Birds Forest 04 Amp.wav   \n",
       "392115                 Snap of fingers   \n",
       "\n",
       "                                              description  \\\n",
       "391277  An other birds singings recorded on the mornin...   \n",
       "392115                            a snap of one's fingers   \n",
       "\n",
       "                                                     tags  \\\n",
       "391277  [birdsong, bird, forest, environment, morning,...   \n",
       "392115      [fingers, finger, 5maudio17, uam, fingersnap]   \n",
       "\n",
       "                                                  license uploader  track_num  \\\n",
       "391277  http://creativecommons.org/publicdomain/zero/1.0/  ANARKYA     391277   \n",
       "392115  http://creativecommons.org/publicdomain/zero/1.0/    edton     392115   \n",
       "\n",
       "          wav_name    png_name  labels_15  labels_2  labels_4  labels  \n",
       "391277  391277.wav  391277.png          4         2         1       2  \n",
       "392115  392115.wav  392115.png          6         8         3       1  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_info.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Begin MODELING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/home/ec2-user/SageMaker/data/'\n",
    "# not uploading audio to sagemaker\n",
    "# audio_dir = '/Users/Mateo/Springboard/FSD50k/data/FSD50K.dev_audio/'\n",
    "train_wav_names = dev_info.wav_name.to_list()\n",
    "train_png_names = dev_info.png_name.to_list()\n",
    "\n",
    "test_wav_names = eval_info.wav_name.to_list()\n",
    "test_png_names = eval_info.png_name.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Old dict\n",
    "\n",
    "# label_dict ={\n",
    "#     0: 'human',\n",
    "#     1: 'comedy',\n",
    "#     2: 'door',\n",
    "#     3: 'baby',\n",
    "#     4: 'animal',\n",
    "#     5: 'music',\n",
    "#     6: 'percussion',\n",
    "#     7: 'machines',\n",
    "#     8: 'ambient',\n",
    "#     9: 'fire',\n",
    "#     10: 'movement',\n",
    "#     11: 'household',\n",
    "#     12: 'foley',\n",
    "#     13: 'nature',\n",
    "#     14: 'other'    \n",
    "# }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sanity Check For Mini Dataframe (2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2000 = dev_info.copy() # no slice!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40966, 12)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_2000.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. CNN from Spectogram (move to first position)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Notes on Architecture:(https://github.com/GorillaBus/urban-audio-classifier/blob/master/3-cnn-model-mfcc.ipynb)\n",
    "    \n",
    "   Convolutional layers and filter amounts: each convolutional layer will learn different features from the training data. The first layers will always learn more low level features (like lines and dots on an image, for example), while the following layers will learn more high-level patterns (basic shapes). The more complex the data is in means of patterns, the more convolutional layers that will be required learn them.\n",
    "On the other hand, there will always be less low-level patterns and more high-level patters (formed by combinations of those lower level patterns), that's why we keep increasing the amount of filters as we add convolutional layers to the network.\n",
    "\n",
    "Kernel sizes: I'm using one 5x5 and 3x3 kernel sizes as the smallest size that worked out on my experiments. I also tested 7x7 -as saw on some image classifiers- but they didn't work well with this data. Also notice I use odd numbers for kernel sizes, this is a very important rule and here is an excellent explanation Stackoverflow.\n",
    "\n",
    "Batch normalization: In short, it normalizes the output of a previous activation layer by subtracting the batch mean and dividing by the batch standard deviation. This process optimizes training time. There is a friendly introduction to batch normalization .\n",
    "\n",
    "MaxPooling: a common technique to downsample our data reducing dimensionality, if not familiarized have a look at this article from computersciencewiki.org.\n",
    "\n",
    "Spatial dropout: A regularization method that works like the standard dropout used in neural networks, that applies better for convolutional layers: it works by dropping out entire feature maps by a given rate, preventing activations from becoming strongly correlated. It's effect is usually more sensible than the standard dropout and as you may have advertised I'm increasing the rate as I add new layers, this is a common practice as higher level layers are also containing more filters. Another important advice is to use it after max pooling.\n",
    "\n",
    "GlobalAveragePooling: until some years ago it was mostly common to see CNN architectures that would flatten out the output from the last convolutional layer to connect it to a series of dense layers that would \"interprete\" and make category predictions. Today is more common to see this dense layers replaced with a Global Average Pooling layer that calculates the average output of each feature map in the previous layer, strongly reducing dimensionality. This demonstrated to work better in many scenarios, also being much more computationally cheap. Read more in this article.\n",
    "\n",
    "Dense (softmax output): the final layer containing the softmax output that will provide the classification probabilities for the input data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes on Keras/ Documentation\n",
    "\n",
    "Conv2D documentation: https://keras.io/api/layers/convolution_layers/convolution2d/\n",
    "MaxPooling2D: https://keras.io/api/layers/pooling_layers/max_pooling2d/\n",
    "Dropout: https://keras.io/api/layers/regularization_layers/dropout/\n",
    "Flatten: https://www.tensorflow.org/api_docs/python/tf/keras/layers/Flatten\n",
    "Dense: https://keras.io/api/layers/core_layers/dense/\n",
    "\n",
    "\n",
    "Conv2D\n",
    "the first number (32) represents 'filters', or how many outputs for this layer (that's why it grows for each Conv2D)\n",
    "- these filters are the same as nodes (maybe not)\n",
    "kernel_size = size of sliding window\n",
    "default: strides = (1,1)\n",
    "default: padding = 'valid' (means 'none', switch to 'same' for equal padding for all kernels/windows)\n",
    "\n",
    "MaxPooling2D:\n",
    "pool_size: dimensions of pixels to analyze, aggegrating by the max value\n",
    "strides = default, switch uses dimensions of pool_size for sliding window (thus does not aggegrate overlapping pixels)\n",
    "padding = default, which is 'valid'(none), 'same' creates equal padding (better no?)\n",
    "\n",
    "Dropout:\n",
    "Fraction of the input units to drop.\n",
    "The Dropout layer randomly sets input units to 0 with a frequency of rate at each step during training time\n",
    "I believe that means that one designates the percentage of input dropped, that are randomly (not evenly) spaced?\n",
    "\n",
    "Flatten\n",
    "takes a multi-dimensioned matrix, and converts it into a 1-d array.\n",
    "i.e. An output from a Conv2D layer with a shape of (1,10,64) passed through Flatten would have a shape of (640,)\n",
    "\n",
    "Dense:\n",
    "Just your regular densely-connected NN layer. \n",
    "-(a fully connected layer? as opposed to a Convutional layer, which is like a sliding window?)\n",
    "units: Positive integer, dimensionality of the output space.\n",
    "activation: Activation function to use. Commonly 'relu' when a binary output is desired\n",
    "-softmax is generally used for the final output layer on multiclassification problems).\n",
    "\n",
    "\n",
    "Optimizer/ Compile:\n",
    "alternate Optimizers in compiling (as opposed to SGD Stochastic Gradient Descent)\n",
    "(note: also use sparse_categorical_crossentropy with numeric input for label in .flow_from_dataframe)\n",
    "\n",
    "optimizer = Adam(lr=1e-4, beta_1=0.99, beta_2=0.999)\n",
    "optimizer = optimizers.Adagrad(lr=0.01, epsilon=None, decay=0.0)\n",
    "optimizer = keras.optimizers.adam()\n",
    "optimizer = optimizers.SGD(lr=0.002, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "\n",
    "\n",
    "\n",
    "Note: Consider adding a Keras BatchNormilization layer (between linear and nonlinear layers, after flatten?)\n",
    "\n",
    "\n",
    "Feature dimensions (MFCC x Time)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loading ONE image from Spectrogram directory "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_info.labels.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # loading Spectrogram (sanity check)\n",
    "\n",
    "# spec_images_dir = data_dir + 'train_spectrograms/'\n",
    "# file_path = spec_images_dir + train_png_names[0]\n",
    "\n",
    "# # loading processed MFCC Spectrogram Images\n",
    "# spec = skimage.io.imread(file_path)\n",
    "# print(spec.shape)\n",
    "# librosa.display.specshow(spec)\n",
    "\n",
    "# plt.ylabel('MFCC Coefficient Bands')\n",
    "# plt.xlabel('Time in 23 millisecond increments')\n",
    "# plt.title('Spectrogram of Audio Wave')\n",
    "# plt.yticks(range(0,128, 24))\n",
    "# plt.xticks(range(0,217, 48))\n",
    "# plt.colorbar()\n",
    "# plt.title('Spectrogram of Audio Wave')\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data into generator with keras function .flow_from_directory\n",
    "\n",
    "datagen=ImageDataGenerator(rescale=1./255.,\n",
    "                          validation_split=0.1)  # , default: validation_split= None\n",
    "# 25% test set before, thats why only 75 before, seperate  eval so validiation =0\n",
    "\n",
    "\n",
    "train_spec_generator=datagen.flow_from_dataframe(\n",
    "    dataframe= df_2000,\n",
    "    directory= 'data/train_spectrograms/',\n",
    "    x_col='png_name',\n",
    "    y_col= 'labels', # previously set as subset=\"training\", use when having validation_split\n",
    "    color_mode=\"grayscale\",\n",
    "    batch_size=32,\n",
    "    seed=42,\n",
    "    shuffle=True,\n",
    "    class_mode=  \"raw\", #  'raw' for numeric input\n",
    "    target_size=(128,216),\n",
    "    subset='training') # target_size=(64,64)) tuple of height/width\n",
    "\n",
    "valid_spec_generator=datagen.flow_from_dataframe(\n",
    "    dataframe= df_2000,\n",
    "    directory= 'data/train_spectrograms',\n",
    "    x_col='png_name',\n",
    "    y_col= 'labels', # previously set as subset=\"training\", use when having validation_split\n",
    "    color_mode=\"grayscale\",\n",
    "    batch_size=32,\n",
    "    seed=42,\n",
    "    shuffle=True,\n",
    "    class_mode=  \"raw\", #  'raw' for numeric input\n",
    "    target_size=(128,216),\n",
    "    subset='validation') # target_size=(64,64)) tuple of height/width\n",
    "\n",
    "# test generator\n",
    "datagen=ImageDataGenerator(rescale=1./255.)\n",
    "\n",
    "test_spec_generator=datagen.flow_from_dataframe(\n",
    "    dataframe= eval_info,\n",
    "    directory= 'data/test_spectrograms',\n",
    "    x_col='png_name',\n",
    "    y_col= 'labels',\n",
    "    color_mode=\"grayscale\",\n",
    "    batch_size=32,\n",
    "    seed=42,\n",
    "    shuffle=True,\n",
    "    class_mode=  \"raw\", # 'raw' for numeric input\n",
    "    target_size=(128,216)) # target_size=(64,64)) tuple of height/width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "num_classes= 8 # dev_info.labels.nunique()\n",
    "\n",
    "#Define Model\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(128,216,1))) # Height, Width, Num_channels\n",
    "# seems like this should be just 1 channel, but asked for three, check this out later\n",
    "model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "# optimizer=tf.keras.optimizers.adam() didnt work...\n",
    "optimizer = optimizers.SGD(lr=0.002, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "\n",
    "\n",
    "#Compile   # had to switch to sparse_categorical_crossentropy from categorical_crossentropy\n",
    "model.compile(loss=keras.losses.sparse_categorical_crossentropy, optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Train and Test The Model\n",
    "model.fit(x=train_spec_generator,validation_data=valid_spec_generator, epochs=3, verbose=1)    # switched to 5 from 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Evaluating Testing Data\n",
    "\n",
    "score = model.evaluate(test_spec_generator)\n",
    "accuracy = 100*score[1]\n",
    "print(\"Testing accuracy: %.2f%%\" % accuracy) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. CNN with MFCC Spectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # loading one MFCC Spectrogram\n",
    "\n",
    "# spec_images_dir = data_dir + 'train_mfcc/'\n",
    "# file_path = spec_images_dir + train_png_names[0]\n",
    "\n",
    "\n",
    "# # loading processed MFCC Spectrogram Images\n",
    "# spec = skimage.io.imread(file_path)\n",
    "# print(spec.shape)\n",
    "# librosa.display.specshow(spec)\n",
    "\n",
    "# plt.ylabel('MFCC Coefficient Bands')\n",
    "# plt.xlabel('Time in 23 millisecond increments')\n",
    "# plt.title('MFCC Spectrogram')\n",
    "# plt.yticks(range(1,33))\n",
    "# plt.xticks(range(0,217, 48))\n",
    "# plt.colorbar()\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 36870 validated image filenames.\n",
      "Found 4096 validated image filenames.\n",
      "Found 10231 validated image filenames.\n"
     ]
    }
   ],
   "source": [
    "# loading MFCC Spectrograms into generator \n",
    "\n",
    "datagen=ImageDataGenerator(rescale=1./255.,\n",
    "                          validation_split=0.1)  \n",
    "\n",
    "\n",
    "mfcc_train_generator=datagen.flow_from_dataframe(\n",
    "    dataframe= df_2000,\n",
    "    directory= 'data/train_mfcc',\n",
    "    x_col='png_name',\n",
    "    y_col= 'labels',\n",
    "    color_mode=\"grayscale\",\n",
    "    batch_size=32,\n",
    "    seed=42,\n",
    "    shuffle=True,\n",
    "    subset='training',\n",
    "    class_mode=\"raw\", \n",
    "    target_size=(32,216)) # target_size=(64,64)) tuple of height/width\n",
    "\n",
    "mfcc_valid_generator=datagen.flow_from_dataframe(\n",
    "    dataframe= df_2000,\n",
    "    directory= 'data/train_mfcc',\n",
    "    x_col='png_name',\n",
    "    y_col= 'labels',\n",
    "    color_mode=\"grayscale\",\n",
    "    batch_size=32,\n",
    "    seed=42,\n",
    "    shuffle=True,\n",
    "    subset='validation',\n",
    "    class_mode=\"raw\",\n",
    "    target_size=(32,216)) #tuple of height/width\n",
    "\n",
    "# test generator\n",
    "datagen=ImageDataGenerator(rescale=1./255.)\n",
    "\n",
    "mfcc_test_generator=datagen.flow_from_dataframe(\n",
    "    dataframe= eval_info,\n",
    "    directory= 'data/test_mfcc',\n",
    "    x_col='png_name',\n",
    "    y_col= 'labels',\n",
    "    color_mode=\"grayscale\",\n",
    "    batch_size=32,\n",
    "    seed=42,\n",
    "    shuffle=True,\n",
    "    class_mode=\"raw\", \n",
    "    target_size=(32,216)) # target_size=(64,64)) tuple of height/width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 31, 215, 32)       160       \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 30, 214, 48)       6192      \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 29, 213, 120)      23160     \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 14, 106, 120)      0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 14, 106, 120)      0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 178080)            0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               22794368  \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 22,832,786\n",
      "Trainable params: 22,832,786\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(2, 2), activation='relu', input_shape=(32, 216, 1))) # Height, Width, num_channels\n",
    "model.add(Conv2D(48, kernel_size=(2, 2), activation='relu'))\n",
    "model.add(Conv2D(120, kernel_size=(2, 2), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Dense(10, activation='softmax'))  # num_classes = 15\n",
    "\n",
    "optimizer = optimizers.SGD(lr=0.002, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "\n",
    "model.compile(loss=keras.losses.sparse_categorical_crossentropy,\n",
    "              optimizer=optimizer,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1153/1153 [==============================] - 1592s 1s/step - loss: 1.3500 - accuracy: 0.5246 - val_loss: 1.3975 - val_accuracy: 0.5107\n",
      "Epoch 4/20\n",
      " 448/1153 [==========>...................] - ETA: 15:27 - loss: 1.3007 - accuracy: 0.5410"
     ]
    }
   ],
   "source": [
    "model.fit(x=mfcc_train_generator, validation_data=mfcc_valid_generator, epochs= 20, verbose=1)    # switched to 5 from 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating the model on the training and testing set\n",
    "\n",
    "score = model.evaluate(mfcc_test_generator, verbose=1)\n",
    "accuracy = 100*score[1]\n",
    "print(\"Testing accuracy: %.2f%%\" % accuracy) \n",
    "\n",
    "# score = model.evaluate(x_test, y_test, verbose=0)\n",
    "# print(\"Testing Accuracy: \", score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. CNN with Mean MFCC Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading Training and Testing Data: Mean MFCC features\n",
    "    \n",
    "train_mfcc_mean = np.load('data/train_mean_mfcc.npz')['arr_0']\n",
    "test_mfcc_mean = np.load('data/test_mean_mfcc.npz')['arr_0']\n",
    "\n",
    "print(train_mfcc_mean.shape)\n",
    "print(test_mfcc_mean.shape)\n",
    "# sample mean_mfcc_array:\n",
    "train_mfcc_mean[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = dev_info['labels'] # to_categorical(dev_info['labels']) # no slice\n",
    "y_test = eval_info['labels'] # to_categorical(eval_info['labels']) # no slice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(32, input_dim=32, activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(15, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the keras model\n",
    "model.compile(loss=keras.losses.sparse_categorical_crossentropy, optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(train_mfcc_mean, y_train, epochs=10, batch_size=50, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating the model on the training and testing set\n",
    "\n",
    "score = model.evaluate(train_mfcc_mean, y_train, verbose=1)\n",
    "accuracy = 100*score[1]\n",
    "print(\"Training accuracy: %.2f%%\" % accuracy) \n",
    "\n",
    "score = model.evaluate(test_mfcc_mean, y_test, verbose=1)\n",
    "accuracy = 100*score[1]\n",
    "print(\"Test accuracy: %.2f%%\" % accuracy) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MFCC CNN without generator\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mfcc = pd.read_json('data/train_mfcc_df.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_mfcc = pd.read_json('data/test_mfcc_df.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_mfcc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_mfcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_mfcc['mfcc']\n",
    "X_test = test_mfcc['mfcc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train_mfcc['labels'] # to_categorical(dev_info['labels']) # no slice\n",
    "y_test = test_mfcc['labels'] # to_categorical(eval_info['labels']) # no slice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(2, 2), activation='relu', input_shape=(32, 216, 1))) # Height, Width, num_channels\n",
    "model.add(Conv2D(48, kernel_size=(2, 2), activation='relu'))\n",
    "model.add(Conv2D(120, kernel_size=(2, 2), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Dense(10, activation='softmax'))  # num_classes = 15\n",
    "\n",
    "optimizer = optimizers.SGD(lr=0.002, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "\n",
    "model.compile(loss=keras.losses.sparse_categorical_crossentropy,\n",
    "              optimizer=optimizer,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=keras.losses.sparse_categorical_crossentropy, optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train, epochs=10, batch_size=50, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note: read some articles on extracting feature importance from this basic neural net\n",
    "### Then maybe a Random Forest Model to then get performance + feat.importance/interpretable results\n",
    "\n",
    "- evaluate model performance by shuffling bands and evaluating affect on model performance (PERMUTATION FEAT. IMPORT)\n",
    "(https://christophm.github.io/interpretable-ml-book/feature-importance.html)\n",
    "\n",
    "- Random Forest Feature Importance (Impurity based)(Warning: impurity-based feature importances can be misleading for high cardinality features (many unique values). See sklearn.inspection.permutation_importance as an alternative.)\n",
    "(https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier.feature_importances_)\n",
    "\n",
    "- XGBoost Classifier Feature Importance (https://stackoverflow.com/questions/37627923/how-to-get-feature-importance-in-xgboost)\n",
    "(https://machinelearningmastery.com/feature-importance-and-feature-selection-with-xgboost-in-python/)\n",
    "-https://mljar.com/blog/feature-importance-xgboost/\n",
    "\n",
    "-(also consider Drop Column Feature Importance)\n",
    "https://explained.ai/rf-importance/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End | Notes Below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This trims silence???? use maybe on longer audio tracks?\n",
    "\n",
    "# trim silent edges\n",
    "\n",
    "whale_song, _ = librosa.effects.trim(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  # labeling pickle files instructions:\n",
    "    \n",
    "    # store extracted training data\n",
    "    training_examples.to_pickle('Extracted_Features\\\\' + training_name + '_features.pkl')\n",
    "    training_labels.to_pickle('Extracted_Features\\\\' + training_name + '_labels.pkl')\n",
    "\n",
    "    # store extracted validation data\n",
    "    validation_examples.to_pickle('Extracted_Features\\\\' + validation_name + '_features.pkl')\n",
    "    validation_labels.to_pickle('Extracted_Features\\\\' + validation_name + '_labels.pkl')\n",
    "    \n",
    "    # import glob\n",
    "# import imageio\n",
    "\n",
    "# for image_path in glob.glob(\"/Users/Mateo/Springboard/FSD50k_stuff/data/spec_100/*.png\"):\n",
    "#     im = imageio.imread(image_path)\n",
    "#     plt.imshow(im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1D CNN Not Applicable for this project (would apply if analyzing straight audio wav files (better for timeseries))\n",
    "\n",
    "# from keras.layers import Conv1D\n",
    "# from keras.layers import MaxPooling1D\n",
    "\n",
    "# # https://keras.io/api/layers/convolution_layers/convolution1d/\n",
    "# # https://keras.io/api/layers/pooling_layers/max_pooling1d/\n",
    "\n",
    "# verbose, epochs, batch_size = 0, 10, 32\n",
    "\n",
    "# n_timesteps, n_features, n_outputs = 1, 13, 15 # no time, 13 mean features, 15 class outputs of model\n",
    "\n",
    "\n",
    "# model = Sequential()\n",
    "# model.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(13)))\n",
    "# model.add(Conv1D(filters=64, kernel_size=3, activation='relu'))\n",
    "# model.add(Dropout(0.5))\n",
    "# model.add(MaxPooling1D(pool_size=2))\n",
    "# model.add(Flatten())\n",
    "# model.add(Dense(100, activation='relu'))\n",
    "# model.add(Dense(n_outputs, activation='softmax'))\n",
    "\n",
    "# model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# model.fit(mfcc_mean_features, y_hot, epochs=epochs, batch_size=batch_size, verbose=verbose)\n",
    "# # evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helpful articles\n",
    "\n",
    "https://towardsdatascience.com/getting-to-know-the-mel-spectrogram-31bca3e2d9d0\n",
    "https://towardsdatascience.com/audio-classification-using-fastai-and-on-the-fly-frequency-transforms-4dbe1b540f89\n",
    "https://pytorch.org/audio/stable/index.html\n",
    "(good explanations of Mel and CNN) https://scottmduda.medium.com/urban-environmental-audio-classification-using-mel-spectrograms-706ee6f8dcc1\n",
    "    \n",
    "(indepth explanation of MFS MFCC)    https://haythamfayek.com/2016/04/21/speech-processing-for-machine-learning.html\n",
    " free course on audio processing   https://www.coursera.org/learn/audio-signal-processing\n",
    "    \n",
    "Urban sound dataset (Consideration, 4seconds in length) https://urbansounddataset.weebly.com/urbansound8k.html\n",
    "https://medium.com/gradientcrescent/urban-sound-classification-using-convolutional-neural-networks-with-keras-theory-and-486e92785df4\n",
    "    \n",
    "    \n",
    "# audio walk through with keras\n",
    "http://aqibsaeed.github.io/2016-09-03-urban-sound-classification-part-1/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
